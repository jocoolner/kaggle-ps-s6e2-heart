{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf09770",
   "metadata": {},
   "source": [
    "load OOF/test arrays + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632a7c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (630000,) (630000,) (270000,) (270000,)\n",
      "AUC cat OOF: 0.9554845267311664\n",
      "AUC lgb OOF: 0.9551498401438248\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "DATA_DIR = ROOT / \"data\" / \"raw\"\n",
    "REPORTS_DIR = ROOT / \"reports\"\n",
    "\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "sub   = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
    "\n",
    "id_col = sub.columns[0]\n",
    "target_col = sub.columns[1]\n",
    "y = (train[target_col] == \"Presence\").astype(int).values\n",
    "\n",
    "oof_cat = np.load(REPORTS_DIR / \"oof_catboost.npy\")\n",
    "test_cat = np.load(REPORTS_DIR / \"test_catboost.npy\")\n",
    "\n",
    "oof_lgb = np.load(REPORTS_DIR / \"oof_lgbm.npy\")\n",
    "test_lgb = np.load(REPORTS_DIR / \"test_lgbm.npy\")\n",
    "\n",
    "print(\"Loaded:\", oof_cat.shape, oof_lgb.shape, test_cat.shape, test_lgb.shape)\n",
    "print(\"AUC cat OOF:\", roc_auc_score(y, oof_cat))\n",
    "print(\"AUC lgb OOF:\", roc_auc_score(y, oof_lgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c359bf9",
   "metadata": {},
   "source": [
    "weight search (prob + rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641a2001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PROB: AUC= 0.955485 w_cat= 0.99\n",
      "Best RANK: AUC= 0.955485 w_cat= 0.975\n"
     ]
    }
   ],
   "source": [
    "def rank01(x: np.ndarray) -> np.ndarray:\n",
    "    r = rankdata(x, method=\"average\")\n",
    "    return r / len(r)\n",
    "\n",
    "def best_weight(oof_a, oof_b, y, mode=\"prob\"):\n",
    "    best_auc = -1.0\n",
    "    best_w = None\n",
    "    for w in np.linspace(0, 1, 201):  # step 0.005\n",
    "        if mode == \"prob\":\n",
    "            pred = w * oof_a + (1 - w) * oof_b\n",
    "        else:\n",
    "            pred = w * rank01(oof_a) + (1 - w) * rank01(oof_b)\n",
    "        auc = roc_auc_score(y, pred)\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_w = w\n",
    "    return best_auc, best_w\n",
    "\n",
    "auc_prob, w_prob = best_weight(oof_cat, oof_lgb, y, mode=\"prob\")\n",
    "auc_rank, w_rank = best_weight(oof_cat, oof_lgb, y, mode=\"rank\")\n",
    "\n",
    "print(\"Best PROB: AUC=\", round(float(auc_prob), 6), \"w_cat=\", round(float(w_prob), 3))\n",
    "print(\"Best RANK: AUC=\", round(float(auc_rank), 6), \"w_cat=\", round(float(w_rank), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffda7be",
   "metadata": {},
   "source": [
    "That result tells us something very clear:\n",
    "\n",
    "CatBoost and LGBM are extremely similar (highly correlated)\n",
    "\n",
    "The best blend is basically CatBoost-only:\n",
    "\n",
    "Prob blend: best at w_cat = 0.99\n",
    "\n",
    "Rank blend: best at w_cat = 0.975\n",
    "\n",
    "And the best blended OOF AUC is exactly the same (0.955485) — meaning blending isn’t adding signal, just tiny noise.\n",
    "\n",
    "So: don’t expect blending to beat your CatBoost GPU LB by much (if at all).\n",
    "Still, it’s worth submitting the best blend once because LB can behave slightly differently than OOF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd797a3",
   "metadata": {},
   "source": [
    "Prob blend (w_cat=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5480f5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Dev\\kaggle-ps-s6e2-heart\\reports\\sub_oofbest_prob_wcat_99.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "DATA_DIR = ROOT / \"data\" / \"raw\"\n",
    "REPORTS_DIR = ROOT / \"reports\"\n",
    "\n",
    "test  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "sub   = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
    "id_col = sub.columns[0]\n",
    "target_col = sub.columns[1]\n",
    "\n",
    "test_cat = np.load(REPORTS_DIR / \"test_catboost.npy\")\n",
    "test_lgb = np.load(REPORTS_DIR / \"test_lgbm.npy\")\n",
    "\n",
    "w_cat = 0.99\n",
    "pred = w_cat * test_cat + (1 - w_cat) * test_lgb\n",
    "\n",
    "out_path = REPORTS_DIR / \"sub_oofbest_prob_wcat_99.csv\"\n",
    "pd.DataFrame({id_col: test[id_col], target_col: pred}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4c789",
   "metadata": {},
   "source": [
    "Rank blend (w_cat=0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c9b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Dev\\kaggle-ps-s6e2-heart\\reports\\sub_oofbest_rank_wcat_97_5.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "DATA_DIR = ROOT / \"data\" / \"raw\"\n",
    "REPORTS_DIR = ROOT / \"reports\"\n",
    "\n",
    "test  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "sub   = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
    "id_col = sub.columns[0]\n",
    "target_col = sub.columns[1]\n",
    "\n",
    "test_cat = np.load(REPORTS_DIR / \"test_catboost.npy\")\n",
    "test_lgb = np.load(REPORTS_DIR / \"test_lgbm.npy\")\n",
    "\n",
    "def rank01(x):\n",
    "    r = rankdata(x, method=\"average\")\n",
    "    return r / len(r)\n",
    "\n",
    "w_cat = 0.975\n",
    "pred = w_cat * rank01(test_cat) + (1 - w_cat) * rank01(test_lgb)\n",
    "\n",
    "out_path = REPORTS_DIR / \"sub_oofbest_rank_wcat_97_5.csv\"\n",
    "pd.DataFrame({id_col: test[id_col], target_col: pred}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
